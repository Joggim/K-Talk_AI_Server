import os
import torch
import torch.nn as nn
import joblib

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
MODEL_PATH = os.path.join(BASE_DIR, "../models/mlp_pronunciation_classifier.pt")
ENCODER_PATH = os.path.join(BASE_DIR, "../models/label_encoder.pkl")

# ğŸ”¡ ìëª¨ ì¸ì½”ë”© í•¨ìˆ˜
def encode_jamo(jamo):
    jamo_to_idx = {
        '': 0, 'ã„±': 1, 'ã„²': 2, 'ã„´': 3, 'ã„·': 4, 'ã„¸': 5, 'ã„¹': 6,
        'ã…': 7, 'ã…‚': 8, 'ã…ƒ': 9, 'ã……': 10, 'ã…†': 11, 'ã…‡': 12,
        'ã…ˆ': 13, 'ã…‰': 14, 'ã…Š': 15, 'ã…‹': 16, 'ã…Œ': 17, 'ã…': 18, 'ã…': 19,
        'ã…': 20, 'ã…': 21, 'ã…‘': 22, 'ã…“': 23, 'ã…•': 24, 'ã…—': 25,
        'ã…›': 26, 'ã…œ': 27, 'ã… ': 28, 'ã…¡': 29, 'ã…£': 30
    }
    return jamo_to_idx.get(jamo, 0)

# ğŸ§  MLP ëª¨ë¸ ì •ì˜
class MLPClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.fc = nn.Sequential(
            nn.Linear(embedding_dim * 5, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )

    def forward(self, x):
        emb = self.embedding(x)
        flat = emb.view(x.size(0), -1)
        return self.fc(flat)

# ğŸ“¦ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° ëª¨ë¸ ë¡œë“œ
VOCAB_SIZE = 52
EMBED_DIM = 64
HIDDEN_DIM = 128
OUTPUT_DIM = 7

model = MLPClassifier(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM, OUTPUT_DIM)
model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device("cpu")))
model.eval()

label_encoder = joblib.load(ENCODER_PATH)

def predict_error_type(target, user, prev, next_, jamo_index_in_syllable):
    encoded = torch.tensor([[
        encode_jamo(target),
        encode_jamo(user),
        encode_jamo(prev),
        encode_jamo(next_),
        jamo_index_in_syllable
    ]], dtype=torch.long)
    with torch.no_grad():
        output = model(encoded)
        pred_idx = output.argmax(1).item()